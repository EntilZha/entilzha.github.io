Title: Entropy and KL Divergence
Date: 2017-2-22
Tags: machine learning
Slug: entropy-and-kl-divergence
Author: Pedro Rodriguez
Description: Intuitive introduction to what the Kl divergence is, how it behaves, and why its useful

### Introduction
The KL divergence is short for the [Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullbackâ€“Leibler_divergence) discovered by Solomon Kullback and Richard Leibler in 1951. Semantically, divergence means the amount by which something is diverging, and diverging in turn means to lie in different directions from a different point. In the case of KL divergence we are interested in how divergent (different) two probability distributions are to each other.

This post will review examples, intuition, and mathematics that are necessary to understand the KL divergence, namely entropy and cross entropy, then do the same for the KL divergence itself.

{% notebook notebooks/kl-divergence.ipynb %}

